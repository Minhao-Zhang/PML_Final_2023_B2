{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 1088.893921\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 976.427307\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 928.484619\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 930.540955\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 925.551758\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 923.899414\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 920.704529\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 923.206787\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 922.762939\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 922.035767\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 925.749268\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 922.328796\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 922.726135\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 919.095581\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 920.513733\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 919.300110\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 922.459839\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 918.875610\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 919.516357\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 920.613647\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 920.736877\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 919.353760\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 921.382568\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 917.001831\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 919.702271\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 918.358215\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 919.150085\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 917.526245\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 919.480347\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 917.271362\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 917.142212\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 920.488953\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 918.584229\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 916.891479\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 914.977783\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 918.177551\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 918.265137\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 915.987183\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 914.806396\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 917.159424\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 915.516357\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 915.266235\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 914.593445\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 911.825623\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 913.211731\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 914.748047\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 915.817627\n",
      "====> Epoch: 1 Average loss: 922.5209\n",
      "====> Test set loss: 913.5419\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 917.599731\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 912.163513\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 914.418091\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 909.126770\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 910.391357\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 915.217285\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 910.660828\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 913.194580\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 908.895874\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 911.543335\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 910.427856\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 912.492310\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 913.188477\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 909.433105\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 909.546936\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 911.572815\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 909.408081\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 912.099548\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 911.078369\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 912.216431\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 911.953247\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 910.427368\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 908.501343\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 911.502319\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 910.695190\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 909.040771\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 909.102905\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 912.151489\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 907.692017\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 907.975403\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 907.727173\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 909.917969\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 908.782837\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 910.821411\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 908.068115\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 905.836304\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 908.636780\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 905.122314\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 906.687988\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 907.463745\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 908.389771\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 909.557434\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 905.744141\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 909.887695\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 908.078735\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 906.505005\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 908.454102\n",
      "====> Epoch: 2 Average loss: 909.8792\n",
      "====> Test set loss: 907.1367\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 908.609802\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 905.524292\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 907.409241\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 904.053162\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 906.550110\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 905.640991\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 907.575684\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 906.217957\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 909.520386\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 906.873535\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 904.711609\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 904.232422\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 905.183838\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 903.779785\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 906.863953\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 908.645264\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 908.442261\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 905.061523\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 907.341309\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 907.070129\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 906.845337\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 904.016479\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 906.414062\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 907.862305\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 907.644409\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 904.175537\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 906.019958\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 903.450562\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 905.404846\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 901.913452\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 903.966370\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 904.146301\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 904.951843\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 903.978882\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 902.393433\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 903.176758\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 902.684753\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 905.716980\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 903.285950\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 901.682800\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 905.003357\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 905.286194\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 902.780151\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 901.148804\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 901.551514\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 903.827026\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 904.084106\n",
      "====> Epoch: 3 Average loss: 905.1105\n",
      "====> Test set loss: 902.5436\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 902.835083\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 901.687256\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 903.733826\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 902.289734\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 900.888000\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 904.883728\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 901.755127\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 901.770020\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 902.665405\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 900.954712\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 901.755798\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 899.746460\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 900.780518\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 903.838989\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 897.822388\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 903.912842\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 901.735840\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 902.701538\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 904.836426\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 901.233215\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 901.489197\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 901.198120\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 901.550354\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 902.115417\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 901.571167\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 903.145264\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 902.037354\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 899.420288\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 903.530640\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 897.500854\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 899.655762\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 900.388245\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 902.368164\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 901.102844\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 900.655029\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 900.775085\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 903.936829\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 901.611145\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 901.461182\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 900.210693\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 902.618652\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 899.448120\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 899.920166\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 900.647705\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 899.631226\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 903.361267\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 901.236267\n",
      "====> Epoch: 4 Average loss: 901.7929\n",
      "====> Test set loss: 900.4593\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 901.880127\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 898.433838\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 898.101685\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 900.047485\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 901.231873\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 899.826477\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 902.020508\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 899.166626\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 898.986023\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 898.705444\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 899.990295\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 898.257568\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 902.084595\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 900.291199\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 896.475037\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 900.207764\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 899.425720\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 900.121704\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 900.984131\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 900.568665\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 899.043457\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 901.515442\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 898.862549\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 902.241699\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 903.758301\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 898.665405\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 902.742981\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 898.926514\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 898.837158\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 900.560669\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 902.447327\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 901.104431\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 898.539246\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 900.549744\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 900.197876\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 900.153870\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 899.341553\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 897.656006\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 900.916931\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 897.095581\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 901.232422\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 897.865540\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 900.391724\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 901.093994\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 900.251221\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 902.106812\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 897.754822\n",
      "====> Epoch: 5 Average loss: 900.3760\n",
      "====> Test set loss: 899.6746\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 901.210449\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 899.639771\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 897.675171\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 898.411804\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 901.701782\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 900.574341\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 898.680298\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 900.131836\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 903.091187\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 898.048950\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 899.186890\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 898.029175\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 896.988892\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 900.026611\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 902.962158\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 898.263733\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 900.999329\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 900.324890\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 899.346436\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 899.731201\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 901.343506\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 898.110474\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 898.376587\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 901.028992\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 899.844971\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 898.777283\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 898.751221\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 901.956299\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 898.049194\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 901.127441\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 899.076294\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 901.566101\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 899.084412\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 899.154541\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 898.811035\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 898.328613\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 897.659363\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 898.431396\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 898.390198\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 902.205566\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 899.896179\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 899.212036\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 900.606628\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 899.086243\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 900.960693\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 899.806396\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 898.315186\n",
      "====> Epoch: 6 Average loss: 899.6333\n",
      "====> Test set loss: 899.0901\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 899.626831\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 899.057983\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 898.561157\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 898.571350\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 898.172241\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 898.956543\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 895.526611\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 897.205933\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 901.900635\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 898.710693\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 898.354248\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 897.639648\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 897.109619\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 897.749084\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 901.114014\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 896.041382\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 896.868896\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 899.202698\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 899.239746\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 899.087036\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 898.046326\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 903.610352\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 898.139038\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 899.736267\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 898.783081\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 900.980347\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 899.184143\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 897.884644\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 899.790161\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 900.238281\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 898.013000\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 902.301758\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 899.788940\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 899.608398\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 900.698730\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 898.492920\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 897.208557\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 896.086060\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 899.545532\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 899.460693\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 898.956299\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 899.553894\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 899.346436\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 899.244812\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 897.215576\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 900.129395\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 897.961304\n",
      "====> Epoch: 7 Average loss: 899.1001\n",
      "====> Test set loss: 898.6148\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 900.565369\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 898.526123\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 897.750244\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 900.144775\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 899.186157\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 896.892212\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 897.231934\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 899.040344\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 900.478760\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 897.888916\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 900.080383\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 898.424072\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 899.381470\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 898.603088\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 898.299805\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 898.708740\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 899.278809\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 898.784180\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 896.841064\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 900.234131\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 897.355347\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 897.735840\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 901.174316\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 899.233521\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 900.041870\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 899.290649\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 896.796326\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 900.098145\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 896.370911\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 900.323242\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 898.421753\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 898.405029\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 899.611450\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 898.333984\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 896.226318\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 897.490173\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 900.935425\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 901.403259\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 899.349731\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 901.387085\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 897.222046\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 900.461670\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 899.055298\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 899.337646\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 901.199341\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 900.906494\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 898.458862\n",
      "====> Epoch: 8 Average loss: 898.6548\n",
      "====> Test set loss: 898.0913\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 896.361572\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 899.131714\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 897.110474\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 895.977295\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 898.181519\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 897.212341\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 901.122131\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 900.612671\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 897.148926\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 899.072632\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 898.399902\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 898.157043\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 898.860657\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 898.899963\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 901.177002\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 898.751587\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 898.408447\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 897.619507\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 899.993164\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 901.824707\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 898.068970\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 898.004578\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 898.596008\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 896.560852\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 898.492188\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 899.935913\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 898.322388\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 897.066528\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 897.950867\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 897.544189\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 898.153442\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 896.838440\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 899.739990\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 896.262939\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 899.906616\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 897.940735\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 899.891968\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 898.349487\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 896.124512\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 897.827637\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 898.463562\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 899.326904\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 899.366455\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 899.027283\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 899.615417\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 899.740356\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 897.514160\n",
      "====> Epoch: 9 Average loss: 898.3603\n",
      "====> Test set loss: 897.8238\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 900.354126\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 897.073853\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 897.957275\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 898.979919\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 897.439026\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 899.069092\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 898.760254\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 897.201843\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 896.851196\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 898.543335\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 899.032471\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 897.411560\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 897.121216\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 897.593262\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 897.876160\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 896.431641\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 899.100830\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 899.111084\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 894.782227\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 896.908691\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 900.418213\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 899.701965\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 897.726440\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 899.052979\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 899.285400\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 898.408203\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 897.340881\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 900.840088\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 897.480957\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 895.853455\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 897.170288\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 897.368652\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 897.468140\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 895.560059\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 898.852600\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 895.746582\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 899.437378\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 900.373291\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 900.869751\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 896.724976\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 897.385132\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 900.221558\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 893.642944\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 897.049255\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 898.435303\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 899.354614\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 896.287109\n",
      "====> Epoch: 10 Average loss: 898.0996\n",
      "====> Test set loss: 897.7013\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "DIM = 20\n",
    "EPOCHS = 10\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "# provided model\n",
    "# class VAE(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(VAE, self).__init__()\n",
    "\n",
    "#         self.fc1 = nn.Linear(784, 400)\n",
    "#         self.fc21 = nn.Linear(400, DIM)\n",
    "#         self.fc22 = nn.Linear(400, DIM)\n",
    "#         self.fc3 = nn.Linear(DIM, 400)\n",
    "#         self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "#     def encode(self, x):\n",
    "#         h1 = F.relu(self.fc1(x))\n",
    "#         return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "#     def reparameterize(self, mu, logvar):\n",
    "#         std = torch.exp(0.5*logvar)\n",
    "#         eps = torch.randn_like(std)\n",
    "#         # temp = torch.distributions.Normal(mu, std).sample()\n",
    "#         return mu + eps*std\n",
    "#         # return temp\n",
    "\n",
    "#     def decode(self, z):\n",
    "#         h3 = F.relu(self.fc3(z))\n",
    "#         return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mu, logvar = self.encode(x.view(-1, 784))\n",
    "#         z = self.reparameterize(mu, logvar)\n",
    "#         return self.decode(z), mu, logvar\n",
    "\n",
    "# TA model in sheet3_solution.ipynb\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, hidden_dims=[300, 50, DIM, 50, 300], data_dim=784):\n",
    "        super().__init__()\n",
    "        assert len(hidden_dims) == 5, \"Insufficiently number of dimensions!\"\n",
    "        self.data_dim = data_dim\n",
    "        self.device = device\n",
    "        # define IO\n",
    "        self.in_layer = nn.Linear(data_dim, hidden_dims[0])\n",
    "        self.out_layer = nn.Linear(hidden_dims[-1], data_dim)\n",
    "        # hidden layer\n",
    "        self.enc_h = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        # define hidden and latent\n",
    "        self.enc_mu = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.enc_sigma = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        # hidden layer decoder\n",
    "        self.dec_h = nn.Linear(hidden_dims[2], hidden_dims[-2])\n",
    "        self.dec_layer = nn.Linear(hidden_dims[-2], hidden_dims[-1])\n",
    "        self.to(device)\n",
    "        \n",
    "    def encode(self, x: torch.Tensor):\n",
    "        h1 = F.relu(self.in_layer(x))\n",
    "        h2 = F.relu(self.enc_h(h1))\n",
    "        return self.enc_mu(h2), self.enc_sigma(h2)\n",
    "\n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        h3 = F.relu(self.dec_h(z))\n",
    "        h4 = F.relu(self.dec_layer(h3))\n",
    "        return torch.sigmoid(self.out_layer(h4))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mu, logvar = self.encode(x.view(-1, self.data_dim))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "# https://github.com/cunningham-lab/cb_and_cc \n",
    "# the following function is copied from the above github repo\n",
    "# where I ported from tensorflow to pytorch\n",
    "def cont_bern_log_norm(lam, l_lim=0.49, u_lim=0.51):\n",
    "    # computes the log normalizing constant of a continuous Bernoulli distribution in a numerically stable way.\n",
    "    # returns the log normalizing constant for lam in (0, l_lim) U (u_lim, 1) and a Taylor approximation in\n",
    "    # [l_lim, u_lim].\n",
    "    # cut_y below might appear useless, but it is important to not evaluate log_norm near 0.5 as torch.where evaluates\n",
    "    # both options, regardless of the value of the condition.\n",
    "    cut_lam = torch.where(torch.logical_or(torch.less(lam, l_lim), torch.greater(lam, u_lim)), lam, l_lim * torch.ones_like(lam))\n",
    "    log_norm = torch.log(torch.abs(2.0 * torch.atanh(1 - 2.0 * cut_lam))) - torch.log(torch.abs(1 - 2.0 * cut_lam))\n",
    "    taylor = np.log(2.0) + 4.0 / 3.0 * torch.pow(lam - 0.5, 2) + 104.0 / 45.0 * torch.pow(lam - 0.5, 4)\n",
    "    return torch.where(torch.logical_or(torch.less(lam, l_lim), torch.greater(lam, u_lim)), log_norm, taylor)\n",
    "\n",
    "# https://github.com/Robert-Aduviri/Continuous-Bernoulli-VAE\n",
    "# the following function is copied from the above github repo \n",
    "# where I did not alter anything\n",
    "def sumlogC( x , eps = 1e-5):\n",
    "    '''\n",
    "    Numerically stable implementation of \n",
    "    sum of logarithm of Continous Bernoulli\n",
    "    constant C, using Taylor 2nd degree approximation\n",
    "        \n",
    "    Parameter\n",
    "    ----------\n",
    "    x : Tensor of dimensions (batch_size, dim)\n",
    "        x takes values in (0,1)\n",
    "    ''' \n",
    "    x = torch.clamp(x, eps, 1.-eps) \n",
    "    mask = torch.abs(x - 0.5).ge(eps)\n",
    "    far = torch.masked_select(x, mask)\n",
    "    close = torch.masked_select(x, ~mask)\n",
    "    far_values =  torch.log( (torch.log(1. - far) - torch.log(far)).div(1. - 2. * far) )\n",
    "    close_values = torch.log(torch.tensor((2.))) + torch.log(1. + torch.pow( 1. - 2. * close, 2)/3. )\n",
    "    return far_values.sum() + close_values.sum()\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum') \n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    logC = sumlogC(recon_x)\n",
    "    # logCC = cont_bern_log_norm(recon_x).sum()\n",
    "    # print(torch.abs(logC - logCC))\n",
    "    # losses.append((BCE.item(), KLD.item(), logCC.item()))\n",
    "    return BCE, KLD, logC\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_loss_vals = []\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        \n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        train_loss_vals.append([temp.item() for temp in loss])\n",
    "        loss = sum(loss)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss))\n",
    "    return np.sum(np.array(train_loss_vals), axis=0) / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_loss_vals = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            \n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            test_loss_vals.append([temp.item() for temp in loss])\n",
    "            loss = sum(loss)\n",
    "            test_loss += loss\n",
    "            \n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                recon_batch = recon_batch.view(128, 1, 28, 28)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                        recon_batch[:n]])\n",
    "                \n",
    "                # save_image(comparison.cpu(),\n",
    "                #            'newresults/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                for i in range(1, 2*n+1):\n",
    "                    ax = plt.subplot(2,n,i)\n",
    "                    plt.imshow(comparison.cpu().detach().numpy()[i-1, 0,:,:], cmap=\"gray\")\n",
    "                    ax.get_xaxis().set_visible(False)\n",
    "                    ax.get_yaxis().set_visible(False)\n",
    "                    ax.margins(0,0)\n",
    "                plt.savefig('cbresults/reconstruction_' + str(epoch) + '.png')\n",
    "                plt.close()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return np.sum(np.array(test_loss_vals), axis=0) / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "train_loss_vals_total = []\n",
    "test_loss_vals_total = []\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train(epoch)\n",
    "    train_loss_vals_total.append(train_loss)\n",
    "    test_loss = test(epoch)\n",
    "    test_loss_vals_total.append(test_loss)\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(64, DIM).to(device)\n",
    "        sample = model.decode(sample).cpu()\n",
    "        save_image(sample.view(64, 1, 28, 28),\n",
    "                    'cbresults/sample_' + str(epoch) + '.png')\n",
    "\n",
    "\n",
    "train_loss_vals_total = np.array(train_loss_vals_total)\n",
    "test_loss_vals_total = np.array(test_loss_vals_total)\n",
    "\n",
    "np.save(\"tmp/cbvae_train_loss_vals_total.npy\", train_loss_vals_total)\n",
    "np.save(\"tmp/cbvae_test_loss_vals_total.npy\", test_loss_vals_total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
